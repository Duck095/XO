{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage import exposure\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng HOG\n",
    "def extract_hog_features(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fd, _ = hog(gray, orientations=9, pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2), visualize=True)  # X√≥a `multichannel`\n",
    "    return fd\n",
    "\n",
    "\n",
    "\n",
    "# C·∫≠p nh·∫≠t l·∫°i h√†m extract_features\n",
    "def extract_features(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "\n",
    "    # Ph√°t hi·ªán ƒë∆∞·ªùng th·∫≥ng\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=100, minLineLength=50, maxLineGap=10)\n",
    "\n",
    "    if lines is None:\n",
    "        return [0, 0, 0]  # Kh√¥ng c√≥ ƒë∆∞·ªùng n√†o ph√°t hi·ªán\n",
    "\n",
    "    # Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng HOG\n",
    "    hog_features = extract_hog_features(image)\n",
    "\n",
    "    # T√≠nh to√°n s·ªë l∆∞·ª£ng ƒë∆∞·ªùng th·∫≥ng v√† kho·∫£ng c√°ch\n",
    "    vertical_lines = sorted([line[0][0] for line in lines if abs(line[0][0] - line[0][2]) < 5])\n",
    "    horizontal_lines = sorted([line[0][1] for line in lines if abs(line[0][1] - line[0][3]) < 5])\n",
    "\n",
    "    avg_v_distance = np.mean(np.diff(vertical_lines)) if len(vertical_lines) > 1 else 0\n",
    "    avg_h_distance = np.mean(np.diff(horizontal_lines)) if len(horizontal_lines) > 1 else 0\n",
    "\n",
    "    # Ki·ªÉm tra k√≠ch th∆∞·ªõc ƒë·∫∑c tr∆∞ng HOG v√† chu·∫©n h√≥a\n",
    "    fixed_hog_size = 81  # K√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh mong mu·ªën c·ªßa ƒë·∫∑c tr∆∞ng HOG\n",
    "    if len(hog_features) < fixed_hog_size:\n",
    "        hog_features = np.pad(hog_features, (0, fixed_hog_size - len(hog_features)), mode='constant')\n",
    "    else:\n",
    "        hog_features = hog_features[:fixed_hog_size]  # C·∫Øt n·∫øu l·ªõn h∆°n\n",
    "\n",
    "    return [len(lines), avg_v_distance, avg_h_distance] + list(hog_features)\n",
    "\n",
    "\n",
    "# H√†m ƒë·ªçc d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c \"board_images/\"\n",
    "def load_data():\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Ki·ªÉm tra xem th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "    if not os.path.exists('board_images'):\n",
    "        print(\"‚ö†Ô∏è Th∆∞ m·ª•c 'board_images' kh√¥ng t·ªìn t·∫°i! H√£y ch·∫°y code t·∫°o d·ªØ li·ªáu tr∆∞·ªõc.\")\n",
    "        return [], [], None\n",
    "\n",
    "    # ƒê·ªçc ·∫£nh t·ª´ th∆∞ m·ª•c\n",
    "    file_list = os.listdir('board_images')\n",
    "    if len(file_list) == 0:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng c√≥ ·∫£nh n√†o trong th∆∞ m·ª•c 'board_images'. H√£y ki·ªÉm tra d·ªØ li·ªáu!\")\n",
    "        return [], [], None\n",
    "\n",
    "    for filename in file_list:\n",
    "        img_path = os.path.join('board_images', filename)\n",
    "        if img_path.endswith(\".png\"):\n",
    "            # ƒê·ªçc ·∫£nh\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            # Ki·ªÉm tra ·∫£nh c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "            if img is None:\n",
    "                print(f\"‚ö†Ô∏è L·ªói khi ƒë·ªçc ·∫£nh: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ ·∫£nh\n",
    "            features = extract_features(img)\n",
    "\n",
    "            # G√°n nh√£n t·ª´ t√™n file (v√≠ d·ª•: \"5x5_0.png\" ‚Üí label = \"5x5\")\n",
    "            label = filename.split('_')[0]  \n",
    "\n",
    "            # L∆∞u ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "            data.append(features)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Ki·ªÉm tra n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu\n",
    "    if len(data) == 0:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c t·∫£i v√†o. Ki·ªÉm tra l·∫°i th∆∞ m·ª•c ·∫£nh!\")\n",
    "        return [], [], None\n",
    "\n",
    "    # Chuy·ªÉn ƒë·ªïi nh√£n th√†nh s·ªë\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "    return np.array(data), np.array(labels), label_encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh: 83.33%\n",
      "\n",
      "üîπ K·∫øt qu·∫£ d·ª± ƒëo√°n (t·ªëi ƒëa 10 k·∫øt qu·∫£):\n",
      "Th·ª±c t·∫ø: 11x11, D·ª± ƒëo√°n: 13x13\n",
      "Th·ª±c t·∫ø: 8x8, D·ª± ƒëo√°n: 8x8\n",
      "Th·ª±c t·∫ø: 14x14, D·ª± ƒëo√°n: 14x14\n",
      "Th·ª±c t·∫ø: 3x3, D·ª± ƒëo√°n: 3x3\n",
      "Th·ª±c t·∫ø: 10x10, D·ª± ƒëo√°n: 10x10\n",
      "Th·ª±c t·∫ø: 6x6, D·ª± ƒëo√°n: 6x6\n",
      "Th·ª±c t·∫ø: 15x15, D·ª± ƒëo√°n: 15x15\n",
      "Th·ª±c t·∫ø: 12x12, D·ª± ƒëo√°n: 11x11\n",
      "Th·ª±c t·∫ø: 11x11, D·ª± ƒëo√°n: 11x11\n",
      "Th·ª±c t·∫ø: 11x11, D·ª± ƒëo√°n: 11x11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SVC(kernel='linear'), LabelEncoder())"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H√†m hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "def train_model():\n",
    "    # ƒê·ªçc d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c \"board_images\"\n",
    "    data, labels, label_encoder = load_data()\n",
    "    \n",
    "    # Ki·ªÉm tra n·∫øu d·ªØ li·ªáu tr·ªëng\n",
    "    if len(data) == 0 or len(labels) == 0:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v√¨ kh√¥ng c√≥ d·ªØ li·ªáu!\")\n",
    "        return\n",
    "    \n",
    "    # Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra (30% d·ªØ li·ªáu cho ki·ªÉm tra)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Hu·∫•n luy·ªán m√¥ h√¨nh SVM v·ªõi kernel tuy·∫øn t√≠nh\n",
    "    model = SVC(kernel='linear')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # ƒê√°nh gi√° m√¥ h√¨nh b·∫±ng ƒë·ªô ch√≠nh x√°c\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"‚úÖ ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n so v·ªõi th·ª±c t·∫ø\n",
    "    print(\"\\nüîπ K·∫øt qu·∫£ d·ª± ƒëo√°n (t·ªëi ƒëa 10 k·∫øt qu·∫£):\")\n",
    "    for i in range(min(len(y_test), 10)):  # Hi·ªÉn th·ªã t·ªëi ƒëa 10 k·∫øt qu·∫£\n",
    "        print(f\"Th·ª±c t·∫ø: {label_encoder.inverse_transform([y_test[i]])[0]}, D·ª± ƒëo√°n: {label_encoder.inverse_transform([y_pred[i]])[0]}\")\n",
    "\n",
    "    # Tr·∫£ v·ªÅ m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán v√† label encoder\n",
    "    return model, label_encoder\n",
    "\n",
    "# Ch·∫°y m√¥ h√¨nh hu·∫•n luy·ªán\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh SVM: 92.22%\n",
      "\n",
      "üîπ K·∫øt qu·∫£ d·ª± ƒëo√°n:\n",
      "Th·ª±c t·∫ø: 11x11, D·ª± ƒëo√°n: 11x11\n",
      "Th·ª±c t·∫ø: 8x8, D·ª± ƒëo√°n: 8x8\n",
      "Th·ª±c t·∫ø: 14x14, D·ª± ƒëo√°n: 14x14\n",
      "Th·ª±c t·∫ø: 3x3, D·ª± ƒëo√°n: 3x3\n",
      "Th·ª±c t·∫ø: 10x10, D·ª± ƒëo√°n: 10x10\n",
      "Th·ª±c t·∫ø: 6x6, D·ª± ƒëo√°n: 6x6\n",
      "Th·ª±c t·∫ø: 15x15, D·ª± ƒëo√°n: 15x15\n",
      "Th·ª±c t·∫ø: 12x12, D·ª± ƒëo√°n: 12x12\n",
      "Th·ª±c t·∫ø: 11x11, D·ª± ƒëo√°n: 11x11\n",
      "Th·ª±c t·∫ø: 11x11, D·ª± ƒëo√°n: 11x11\n"
     ]
    }
   ],
   "source": [
    "def train_model_svm():\n",
    "    # ƒê·ªçc d·ªØ li·ªáu\n",
    "    data, labels, label_encoder = load_data()\n",
    "\n",
    "    if len(data) == 0 or len(labels) == 0:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v√¨ kh√¥ng c√≥ d·ªØ li·ªáu!\")\n",
    "        return\n",
    "\n",
    "    # Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # T√¨m ki·∫øm tham s·ªë t·ªëi ∆∞u cho SVM\n",
    "    parameters = {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10], 'gamma': [0.1, 1, 'scale']}\n",
    "    grid_search = GridSearchCV(SVC(), parameters, cv=5)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # S·ª≠ d·ª•ng m√¥ h√¨nh t·ªët nh·∫•t\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"‚úÖ ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh SVM: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
    "    print(\"\\nüîπ K·∫øt qu·∫£ d·ª± ƒëo√°n:\")\n",
    "    for i in range(min(len(y_test), 10)):  # Hi·ªÉn th·ªã t·ªëi ƒëa 10 k·∫øt qu·∫£\n",
    "        print(f\"Th·ª±c t·∫ø: {label_encoder.inverse_transform([y_test[i]])[0]}, D·ª± ƒëo√°n: {label_encoder.inverse_transform([y_pred[i]])[0]}\")\n",
    "\n",
    "train_model_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"‚è±Ô∏è Th·ªùi gian hu·∫•n luy·ªán: {elapsed_time:.2f} gi√¢y\")\n",
    "\n",
    "print(f\"S·ªë m·∫´u trong t·∫≠p ki·ªÉm tra: {len(X_test)}\")\n",
    "\n",
    "import time\n",
    "\n",
    "# ƒêo th·ªùi gian x·ª≠ l√Ω\n",
    "start_time = time.time()\n",
    "\n",
    "# D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# T√≠nh th·ªùi gian trung b√¨nh x·ª≠ l√Ω m·ªói m·∫´u\n",
    "avg_time_per_sample = elapsed_time / len(X_test)\n",
    "\n",
    "print(f\"‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω cho {len(X_test)} m·∫´u: {elapsed_time:.2f} gi√¢y\")\n",
    "print(f\"‚è±Ô∏è Th·ªùi gian trung b√¨nh x·ª≠ l√Ω m·ªói m·∫´u: {avg_time_per_sample:.4f} gi√¢y\")\n",
    "\n",
    "for i in range(min(len(y_test), 10)):\n",
    "    print(f\"Th·ª±c t·∫ø: {label_encoder.inverse_transform([y_test[i]])[0]}, D·ª± ƒëo√°n: {label_encoder.inverse_transform([y_pred[i]])[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Th∆∞ m·ª•c 'board_images' kh√¥ng t·ªìn t·∫°i! H√£y ch·∫°y code t·∫°o d·ªØ li·ªáu tr∆∞·ªõc.\n",
      "‚ùå Kh√¥ng th·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v√¨ kh√¥ng c√≥ d·ªØ li·ªáu!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "def train_model_svm():\n",
    "    # ƒê·ªçc d·ªØ li·ªáu\n",
    "    data, labels, label_encoder = load_data()\n",
    "\n",
    "    if len(data) == 0 or len(labels) == 0:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v√¨ kh√¥ng c√≥ d·ªØ li·ªáu!\")\n",
    "        return\n",
    "\n",
    "    # Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # T√¨m ki·∫øm tham s·ªë t·ªëi ∆∞u cho SVM\n",
    "    parameters = {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10], 'gamma': [0.1, 1, 'scale']}\n",
    "    grid_search = GridSearchCV(SVC(), parameters, cv=5)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # S·ª≠ d·ª•ng m√¥ h√¨nh t·ªët nh·∫•t\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # L∆∞u m√¥ h√¨nh ra file\n",
    "    joblib.dump(best_model, \"svm_caro_model.pkl\")\n",
    "    print(\"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng v√†o 'svm_caro_model.pkl'\")\n",
    "\n",
    "    # D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"‚úÖ ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh SVM: {accuracy * 100:.2f}%\")\n",
    "\n",
    "train_model_svm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassifier' object has no attribute 'base_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_test), \u001b[38;5;241m10\u001b[39m)):  \u001b[38;5;66;03m# Hi·ªÉn th·ªã t·ªëi ƒëa 10 k·∫øt qu·∫£\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTh·ª±c t·∫ø: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_encoder\u001b[38;5;241m.\u001b[39minverse_transform([y_test[i]])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, D·ª± ƒëo√°n: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_encoder\u001b[38;5;241m.\u001b[39minverse_transform([y_pred[i]])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m \u001b[43mtrain_model_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mtrain_model_rf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# S·ª≠ d·ª•ng m√¥ h√¨nh t·ªët nh·∫•t\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_estimator_\u001b[49m\n\u001b[0;32m     19\u001b[0m  \u001b[38;5;66;03m# L∆∞u m√¥ h√¨nh ra file\u001b[39;00m\n\u001b[0;32m     20\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(best_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvm_caro_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'base_estimator_'"
     ]
    }
   ],
   "source": [
    "def train_model_rf():\n",
    "    # ƒê·ªçc d·ªØ li·ªáu\n",
    "    data, labels, label_encoder = load_data()\n",
    "   \n",
    "    if len(data) == 0 or len(labels) == 0:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v√¨ kh√¥ng c√≥ d·ªØ li·ªáu!\")\n",
    "        return\n",
    "\n",
    "    # Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # S·ª≠ d·ª•ng m√¥ h√¨nh t·ªët nh·∫•t\n",
    "    best_model = rf_model.base_estimator_\n",
    "\n",
    "     # L∆∞u m√¥ h√¨nh ra file\n",
    "    joblib.dump(best_model, \"svm_caro_model.pkl\")\n",
    "    print(\"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng v√†o 'svm_caro_model.pkl'\")\n",
    "\n",
    "    # D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"‚úÖ ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh Random Forest: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
    "    print(\"\\nüîπ K·∫øt qu·∫£ d·ª± ƒëo√°n:\")\n",
    "    for i in range(min(len(y_test), 10)):  # Hi·ªÉn th·ªã t·ªëi ƒëa 10 k·∫øt qu·∫£\n",
    "        print(f\"Th·ª±c t·∫ø: {label_encoder.inverse_transform([y_test[i]])[0]}, D·ª± ƒëo√°n: {label_encoder.inverse_transform([y_pred[i]])[0]}\")\n",
    "\n",
    "\n",
    "train_model_rf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh Decision Tree: 91.11%\n",
      "\n",
      "üîπ K·∫øt qu·∫£ d·ª± ƒëo√°n:\n",
      "Th·ª±c t·∫ø: 11x11, D·ª± ƒëo√°n: 11x11\n",
      "Th·ª±c t·∫ø: 8x8, D·ª± ƒëo√°n: 8x8\n",
      "Th·ª±c t·∫ø: 14x14, D·ª± ƒëo√°n: 14x14\n",
      "Th·ª±c t·∫ø: 3x3, D·ª± ƒëo√°n: 3x3\n",
      "Th·ª±c t·∫ø: 10x10, D·ª± ƒëo√°n: 10x10\n",
      "Th·ª±c t·∫ø: 6x6, D·ª± ƒëo√°n: 6x6\n",
      "Th·ª±c t·∫ø: 15x15, D·ª± ƒëo√°n: 15x15\n",
      "Th·ª±c t·∫ø: 12x12, D·ª± ƒëo√°n: 12x12\n",
      "Th·ª±c t·∫ø: 11x11, D·ª± ƒëo√°n: 11x11\n",
      "Th·ª±c t·∫ø: 11x11, D·ª± ƒëo√°n: 11x11\n"
     ]
    }
   ],
   "source": [
    "def train_model_dt():\n",
    "    # ƒê·ªçc d·ªØ li·ªáu\n",
    "    data, labels, label_encoder = load_data()\n",
    "\n",
    "    if len(data) == 0 or len(labels) == 0:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v√¨ kh√¥ng c√≥ d·ªØ li·ªáu!\")\n",
    "        return\n",
    "\n",
    "    # Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Hu·∫•n luy·ªán m√¥ h√¨nh Decision Tree\n",
    "    dt_model = DecisionTreeClassifier(random_state=42)\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    \n",
    "    # D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra\n",
    "    y_pred = dt_model.predict(X_test)\n",
    "    \n",
    "    # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"‚úÖ ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh Decision Tree: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
    "    print(\"\\nüîπ K·∫øt qu·∫£ d·ª± ƒëo√°n:\")\n",
    "    for i in range(min(len(y_test), 10)):  # Hi·ªÉn th·ªã t·ªëi ƒëa 10 k·∫øt qu·∫£\n",
    "        print(f\"Th·ª±c t·∫ø: {label_encoder.inverse_transform([y_test[i]])[0]}, D·ª± ƒëo√°n: {label_encoder.inverse_transform([y_pred[i]])[0]}\")\n",
    "\n",
    "train_model_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Ki·ªÉm tra s·ªë l∆∞·ª£ng ·∫£nh trong th∆∞ m·ª•c\n",
    "def check_data_distribution():\n",
    "    label_counts = Counter()\n",
    "    \n",
    "    for filename in os.listdir('board_images'):\n",
    "        if filename.endswith('.png'):\n",
    "            label = filename.split('_')[0]  # L·∫•y nh√£n t·ª´ t√™n file\n",
    "            label_counts[label] += 1\n",
    "\n",
    "    print(\"üìä Ph√¢n b·ªë d·ªØ li·ªáu hu·∫•n luy·ªán:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"  - {label}: {count} ·∫£nh\")\n",
    "\n",
    "# Ch·∫°y ki·ªÉm tra d·ªØ li·ªáu\n",
    "check_data_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî∑ Hu·∫•n luy·ªán m√¥ h√¨nh SVM m·∫∑c ƒë·ªãnh...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (900,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 135\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müî∑ Hu·∫•n luy·ªán m√¥ h√¨nh SVM m·∫∑c ƒë·ªãnh...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 135\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msvm_default\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müî∑ Hu·∫•n luy·ªán m√¥ h√¨nh SVM (tuning tham s·ªë)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m     train_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvm_tuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 90\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_model\u001b[39m(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvm_default\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# ƒê·ªçc d·ªØ li·ªáu\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m     data, labels, label_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå Kh√¥ng th·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v√¨ kh√¥ng c√≥ d·ªØ li·ªáu!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 85\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m     83\u001b[0m labels \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(labels)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(labels), label_encoder\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (900,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# H√†m tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng HOG\n",
    "def extract_hog_features(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Chuy·ªÉn ·∫£nh sang grayscale\n",
    "    fd, _ = hog(gray, orientations=9, pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2), visualize=True)  # X√ìA `channel_axis=-1`\n",
    "    return fd\n",
    "\n",
    "\n",
    "\n",
    "# H√†m tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ ·∫£nh\n",
    "def extract_features(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "\n",
    "    # Ph√°t hi·ªán ƒë∆∞·ªùng th·∫≥ng\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=100, minLineLength=50, maxLineGap=10)\n",
    "\n",
    "    if lines is None:\n",
    "        return [0, 0, 0]  # Kh√¥ng c√≥ ƒë∆∞·ªùng n√†o ph√°t hi·ªán\n",
    "\n",
    "    # Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng HOG\n",
    "    hog_features = extract_hog_features(image)\n",
    "\n",
    "    # T√≠nh to√°n s·ªë l∆∞·ª£ng ƒë∆∞·ªùng th·∫≥ng v√† kho·∫£ng c√°ch\n",
    "    vertical_lines = sorted([line[0][0] for line in lines if abs(line[0][0] - line[0][2]) < 5])\n",
    "    horizontal_lines = sorted([line[0][1] for line in lines if abs(line[0][1] - line[0][3]) < 5])\n",
    "\n",
    "    avg_v_distance = np.mean(np.diff(vertical_lines)) if len(vertical_lines) > 1 else 0\n",
    "    avg_h_distance = np.mean(np.diff(horizontal_lines)) if len(horizontal_lines) > 1 else 0\n",
    "\n",
    "    return [len(lines), avg_v_distance, avg_h_distance] + list(hog_features)\n",
    "\n",
    "# H√†m ƒë·ªçc d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c \"board_images/\"\n",
    "def load_data():\n",
    "    data, labels = [], []\n",
    "    \n",
    "    # Ki·ªÉm tra xem th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "    if not os.path.exists('board_images/'):\n",
    "        print(\"‚ö†Ô∏è Th∆∞ m·ª•c 'board_images' kh√¥ng t·ªìn t·∫°i! H√£y ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.\")\n",
    "        return [], [], None\n",
    "\n",
    "    # ƒê·ªçc ·∫£nh t·ª´ th∆∞ m·ª•c\n",
    "    file_list = os.listdir('board_images/')\n",
    "    if len(file_list) == 0:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng c√≥ ·∫£nh n√†o trong th∆∞ m·ª•c 'board_images'. H√£y ki·ªÉm tra d·ªØ li·ªáu!\")\n",
    "        return [], [], None\n",
    "\n",
    "    for filename in file_list:\n",
    "        img_path = os.path.join('board_images', filename)\n",
    "        if img_path.endswith(\".png\"):\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            if img is None:\n",
    "                print(f\"‚ö†Ô∏è L·ªói khi ƒë·ªçc ·∫£nh: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng\n",
    "            features = extract_features(img)\n",
    "            label = filename.split('_')[0]  # V√≠ d·ª•: \"5x5_0.png\" ‚Üí label = \"5x5\"\n",
    "\n",
    "            data.append(features)\n",
    "            labels.append(label)\n",
    "\n",
    "    if len(data) == 0:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c t·∫£i v√†o. Ki·ªÉm tra l·∫°i th∆∞ m·ª•c ·∫£nh!\")\n",
    "        return [], [], None\n",
    "\n",
    "    # Chuy·ªÉn ƒë·ªïi nh√£n th√†nh s·ªë\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "    return np.array(data), np.array(labels), label_encoder\n",
    "\n",
    "# H√†m hu·∫•n luy·ªán chung\n",
    "def train_model(model_name=\"svm_default\"):\n",
    "    # ƒê·ªçc d·ªØ li·ªáu\n",
    "    data, labels, label_encoder = load_data()\n",
    "\n",
    "    if len(data) == 0 or len(labels) == 0:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v√¨ kh√¥ng c√≥ d·ªØ li·ªáu!\")\n",
    "        return\n",
    "\n",
    "    # Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Ch·ªçn m√¥ h√¨nh\n",
    "    if model_name == \"svm_default\":\n",
    "        model = SVC(kernel='linear')\n",
    "    elif model_name == \"svm_tuned\":\n",
    "        parameters = {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10], 'gamma': [0.1, 1, 'scale']}\n",
    "        grid_search = GridSearchCV(SVC(), parameters, cv=5)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        model = grid_search.best_estimator_\n",
    "    elif model_name == \"random_forest\":\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif model_name == \"decision_tree\":\n",
    "        model = DecisionTreeClassifier(random_state=42)\n",
    "    else:\n",
    "        print(\"‚ùå M√¥ h√¨nh kh√¥ng h·ª£p l·ªá! Vui l√≤ng ch·ªçn 'svm_default', 'svm_tuned', 'random_forest', ho·∫∑c 'decision_tree'.\")\n",
    "        return\n",
    "\n",
    "    # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"‚úÖ ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh {model_name}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
    "    print(\"\\nüîπ K·∫øt qu·∫£ d·ª± ƒëo√°n:\")\n",
    "    for i in range(min(len(y_test), 10)):  # Hi·ªÉn th·ªã t·ªëi ƒëa 10 k·∫øt qu·∫£\n",
    "        print(f\"Th·ª±c t·∫ø: {label_encoder.inverse_transform([y_test[i]])[0]}, D·ª± ƒëo√°n: {label_encoder.inverse_transform([y_pred[i]])[0]}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Hu·∫•n luy·ªán t·ª´ng m√¥ h√¨nh\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nüî∑ Hu·∫•n luy·ªán m√¥ h√¨nh SVM m·∫∑c ƒë·ªãnh...\")\n",
    "    train_model(\"svm_default\")\n",
    "\n",
    "    print(\"\\nüî∑ Hu·∫•n luy·ªán m√¥ h√¨nh SVM (tuning tham s·ªë)...\")\n",
    "    train_model(\"svm_tuned\")\n",
    "\n",
    "    print(\"\\nüî∑ Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest...\")\n",
    "    train_model(\"random_forest\")\n",
    "\n",
    "    print(\"\\nüî∑ Hu·∫•n luy·ªán m√¥ h√¨nh Decision Tree...\")\n",
    "    train_model(\"decision_tree\")                                                                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
